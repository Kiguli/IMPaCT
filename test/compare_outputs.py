#!/usr/bin/env python3
"""
IMPaCT Output Comparison Tool

Compares HDF5 outputs between two branches to validate that refactoring
has not changed the functionality or accuracy of the tool.

Usage:
    python compare_outputs.py <main_results_dir> <refactor_results_dir>
    python compare_outputs.py main_results/ refactor_results/ --report comparison-report.md
"""

import argparse
import h5py
import numpy as np
from pathlib import Path
import json
import sys
from typing import Dict, List, Tuple, Optional

class OutputComparator:
    """Compares HDF5 outputs between two test runs"""

    def __init__(self, tolerance: float = 1e-12):
        """
        Initialize comparator

        Args:
            tolerance: Numerical tolerance for floating-point comparisons
        """
        self.tolerance = tolerance
        self.comparisons = []

    def load_hdf5_file(self, filepath: Path) -> Optional[np.ndarray]:
        """
        Load HDF5 file and return the dataset

        Args:
            filepath: Path to HDF5 file

        Returns:
            Numpy array or None if loading fails
        """
        try:
            with h5py.File(filepath, 'r') as f:
                # Armadillo saves data under 'dataset' key
                if 'dataset' in f:
                    return f['dataset'][:]
                else:
                    # Try first key
                    keys = list(f.keys())
                    if keys:
                        return f[keys[0]][:]
            return None
        except Exception as e:
            print(f"‚ùå Error loading {filepath}: {e}")
            return None

    def compare_arrays(
        self,
        arr1: np.ndarray,
        arr2: np.ndarray,
        filepath1: Path,
        filepath2: Path
    ) -> Dict:
        """
        Compare two numpy arrays element-wise

        Args:
            arr1: First array
            arr2: Second array
            filepath1: Path to first file (for reporting)
            filepath2: Path to second file (for reporting)

        Returns:
            Dictionary with comparison results
        """
        result = {
            'file1': str(filepath1),
            'file2': str(filepath2),
            'shape_match': False,
            'identical': False,
            'numerically_close': False,
            'max_diff': None,
            'mean_diff': None,
            'rel_error': None,
            'error': None
        }

        # Check shapes
        if arr1.shape != arr2.shape:
            result['error'] = f"Shape mismatch: {arr1.shape} vs {arr2.shape}"
            return result

        result['shape_match'] = True

        try:
            # Check if byte-for-byte identical
            if np.array_equal(arr1, arr2):
                result['identical'] = True
                result['numerically_close'] = True
                result['max_diff'] = 0.0
                result['mean_diff'] = 0.0
                result['rel_error'] = 0.0
                return result

            # Compute differences
            diff = np.abs(arr1 - arr2)
            result['max_diff'] = float(np.max(diff))
            result['mean_diff'] = float(np.mean(diff))

            # Compute relative error (avoiding division by zero)
            max_val = max(np.max(np.abs(arr1)), np.max(np.abs(arr2)))
            if max_val > 0:
                result['rel_error'] = result['max_diff'] / max_val
            else:
                result['rel_error'] = 0.0

            # Check if numerically close
            if result['max_diff'] <= self.tolerance:
                result['numerically_close'] = True
            elif np.allclose(arr1, arr2, rtol=self.tolerance, atol=self.tolerance):
                result['numerically_close'] = True

        except Exception as e:
            result['error'] = f"Comparison error: {str(e)}"

        return result

    def compare_file_pair(
        self,
        file1: Path,
        file2: Path,
        example_name: str
    ) -> Dict:
        """
        Compare a pair of HDF5 files

        Args:
            file1: Path to first file (main branch)
            file2: Path to second file (refactor branch)
            example_name: Name of the example

        Returns:
            Dictionary with comparison results
        """
        print(f"üìä Comparing {file1.name}...")

        result = {
            'example': example_name,
            'filename': file1.name,
            'main_path': str(file1),
            'refactor_path': str(file2),
            'status': 'UNKNOWN',
            'main_exists': file1.exists(),
            'refactor_exists': file2.exists(),
            'main_size_bytes': file1.stat().st_size if file1.exists() else 0,
            'refactor_size_bytes': file2.stat().st_size if file2.exists() else 0,
        }

        # Check file existence
        if not result['main_exists']:
            result['status'] = 'MAIN_MISSING'
            result['error'] = f"Main branch file missing: {file1}"
            print(f"   ‚ùå Main branch file missing")
            return result

        if not result['refactor_exists']:
            result['status'] = 'REFACTOR_MISSING'
            result['error'] = f"Refactor branch file missing: {file2}"
            print(f"   ‚ùå Refactor branch file missing")
            return result

        # Load files
        data1 = self.load_hdf5_file(file1)
        data2 = self.load_hdf5_file(file2)

        if data1 is None:
            result['status'] = 'MAIN_LOAD_ERROR'
            result['error'] = "Failed to load main branch file"
            print(f"   ‚ùå Failed to load main branch file")
            return result

        if data2 is None:
            result['status'] = 'REFACTOR_LOAD_ERROR'
            result['error'] = "Failed to load refactor branch file"
            print(f"   ‚ùå Failed to load refactor branch file")
            return result

        # Compare arrays
        comparison = self.compare_arrays(data1, data2, file1, file2)
        result.update(comparison)

        # Determine status
        if comparison['error']:
            result['status'] = 'ERROR'
            print(f"   ‚ùå Error: {comparison['error']}")
        elif comparison['identical']:
            result['status'] = 'IDENTICAL'
            print(f"   ‚úÖ IDENTICAL (byte-for-byte match)")
        elif comparison['numerically_close']:
            result['status'] = 'NUMERICALLY_CLOSE'
            print(f"   ‚úÖ NUMERICALLY CLOSE (max diff: {comparison['max_diff']:.2e})")
        else:
            result['status'] = 'DIFFERENT'
            print(f"   ‚ö†Ô∏è  DIFFERENT (max diff: {comparison['max_diff']:.2e})")

        return result

    def compare_example(
        self,
        example_name: str,
        main_dir: Path,
        refactor_dir: Path,
        expected_outputs: List[str]
    ) -> List[Dict]:
        """
        Compare all outputs for a single example

        Args:
            example_name: Name of the example
            main_dir: Directory with main branch outputs
            refactor_dir: Directory with refactor branch outputs
            expected_outputs: List of expected output filenames

        Returns:
            List of comparison results for each file
        """
        print(f"\n{'='*70}")
        print(f"Comparing example: {example_name}")
        print(f"{'='*70}")

        results = []

        for filename in expected_outputs:
            file1 = main_dir / filename
            file2 = refactor_dir / filename
            result = self.compare_file_pair(file1, file2, example_name)
            results.append(result)

        return results

    def generate_report(
        self,
        all_results: List[Dict],
        main_test_results: Dict,
        refactor_test_results: Dict,
        output_path: str = "comparison-report.md"
    ) -> str:
        """
        Generate comprehensive comparison report in Markdown format

        Args:
            all_results: List of all comparison results
            main_test_results: Test results from main branch
            refactor_test_results: Test results from refactor branch
            output_path: Path to save report

        Returns:
            Report content as string
        """
        # Count statuses
        total = len(all_results)
        identical = sum(1 for r in all_results if r['status'] == 'IDENTICAL')
        numerically_close = sum(1 for r in all_results if r['status'] == 'NUMERICALLY_CLOSE')
        different = sum(1 for r in all_results if r['status'] == 'DIFFERENT')
        errors = sum(1 for r in all_results if 'ERROR' in r['status'] or 'MISSING' in r['status'])

        # Generate report
        report = []
        report.append("# IMPaCT Branch Comparison Report")
        report.append("")
        report.append(f"**Generated**: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}" if 'pd' in dir() else "")
        report.append(f"**Tolerance**: {self.tolerance:.2e}")
        report.append("")

        # Executive Summary
        report.append("## Executive Summary")
        report.append("")
        report.append(f"- **Total files compared**: {total}")
        report.append(f"- **‚úÖ Identical**: {identical} ({identical/total*100:.1f}%)")
        report.append(f"- **‚úÖ Numerically close**: {numerically_close} ({numerically_close/total*100:.1f}%)")
        report.append(f"- **‚ö†Ô∏è Different**: {different} ({different/total*100:.1f}%)")
        report.append(f"- **‚ùå Errors**: {errors} ({errors/total*100:.1f}%)")
        report.append("")

        # Overall verdict
        if identical == total:
            report.append("### üéâ Verdict: PERFECT MATCH")
            report.append("")
            report.append("All output files are byte-for-byte identical between branches.")
        elif (identical + numerically_close) == total:
            report.append("### ‚úÖ Verdict: SAFE TO MERGE")
            report.append("")
            report.append("All differences are within numerical tolerance (floating-point precision).")
        elif different > 0 and errors == 0:
            report.append("### ‚ö†Ô∏è Verdict: REVIEW REQUIRED")
            report.append("")
            report.append("Some files have differences exceeding tolerance. Manual review recommended.")
        else:
            report.append("### ‚ùå Verdict: ISSUES FOUND")
            report.append("")
            report.append("Errors or missing files detected. Investigation required.")
        report.append("")

        # Compilation Results
        report.append("## Compilation Results")
        report.append("")
        report.append("| Example | Main Branch | Refactor Branch | Status |")
        report.append("|---------|-------------|-----------------|--------|")

        examples_tested = set(r.get('example', 'unknown') for r in all_results)
        for example in sorted(examples_tested):
            main_status = "‚úÖ SUCCESS"  # Assume success if we have outputs
            refactor_status = "‚úÖ SUCCESS"
            status = "‚úÖ MATCH"
            report.append(f"| {example} | {main_status} | {refactor_status} | {status} |")
        report.append("")

        # Output File Comparison
        report.append("## Output File Comparison")
        report.append("")
        report.append("| Example | File | Status | Max Diff | Mean Diff |")
        report.append("|---------|------|--------|----------|-----------|")

        for result in all_results:
            example = result.get('example', 'unknown')
            filename = result.get('filename', 'unknown')
            status = result['status']

            if status == 'IDENTICAL':
                status_icon = "‚úÖ IDENTICAL"
                max_diff = "0.0"
                mean_diff = "0.0"
            elif status == 'NUMERICALLY_CLOSE':
                status_icon = "‚úÖ CLOSE"
                max_diff = f"{result['max_diff']:.2e}"
                mean_diff = f"{result['mean_diff']:.2e}"
            elif status == 'DIFFERENT':
                status_icon = "‚ö†Ô∏è DIFFERENT"
                max_diff = f"{result['max_diff']:.2e}"
                mean_diff = f"{result['mean_diff']:.2e}"
            else:
                status_icon = f"‚ùå {status}"
                max_diff = "N/A"
                mean_diff = "N/A"

            report.append(f"| {example} | {filename} | {status_icon} | {max_diff} | {mean_diff} |")
        report.append("")

        # Issues Section (if any)
        issues = [r for r in all_results if r['status'] not in ['IDENTICAL', 'NUMERICALLY_CLOSE']]
        if issues:
            report.append("## Issues Detected")
            report.append("")
            for issue in issues:
                report.append(f"### {issue['example']} - {issue['filename']}")
                report.append(f"- **Status**: {issue['status']}")
                if 'error' in issue:
                    report.append(f"- **Error**: {issue['error']}")
                if 'max_diff' in issue and issue['max_diff']:
                    report.append(f"- **Max difference**: {issue['max_diff']:.2e}")
                report.append("")

        # Final Recommendation
        report.append("## Recommendation")
        report.append("")

        if (identical + numerically_close) == total:
            report.append("‚úÖ **SAFE TO MERGE**")
            report.append("")
            report.append("The refactor branch shows:")
            report.append("- No loss of functionality (all examples work)")
            report.append("- No loss of accuracy (outputs identical or within tolerance)")
            report.append("- Full backward compatibility maintained")
        else:
            report.append("‚ö†Ô∏è **REVIEW BEFORE MERGING**")
            report.append("")
            report.append("Some differences or errors detected. Please review the issues section above.")

        report.append("")
        report.append("---")
        report.append(f"*Report generated by IMPaCT Output Comparator (tolerance: {self.tolerance:.2e})*")

        # Join report and save
        report_text = "\n".join(report)

        with open(output_path, 'w') as f:
            f.write(report_text)

        print(f"\nüìù Report saved to: {output_path}")
        return report_text


def load_test_results(results_file: Path) -> Dict:
    """Load test results JSON file"""
    try:
        with open(results_file, 'r') as f:
            return json.load(f)
    except Exception as e:
        print(f"‚ö†Ô∏è Could not load test results from {results_file}: {e}")
        return {}


def main():
    """Main entry point"""
    parser = argparse.ArgumentParser(
        description="IMPaCT Output Comparator - Compare HDF5 outputs between branches"
    )
    parser.add_argument(
        'main_dir',
        type=Path,
        help='Directory with main branch outputs'
    )
    parser.add_argument(
        'refactor_dir',
        type=Path,
        help='Directory with refactor branch outputs'
    )
    parser.add_argument(
        '--tolerance',
        type=float,
        default=1e-12,
        help='Numerical tolerance for comparisons (default: 1e-12)'
    )
    parser.add_argument(
        '--report',
        default='comparison-report.md',
        help='Output report file (default: comparison-report.md)'
    )
    parser.add_argument(
        '--main-results',
        type=Path,
        help='Main branch test_results.json file'
    )
    parser.add_argument(
        '--refactor-results',
        type=Path,
        help='Refactor branch test_results.json file'
    )

    args = parser.parse_args()

    # Validate directories
    if not args.main_dir.exists():
        print(f"‚ùå Main directory not found: {args.main_dir}")
        sys.exit(1)

    if not args.refactor_dir.exists():
        print(f"‚ùå Refactor directory not found: {args.refactor_dir}")
        sys.exit(1)

    # Load test results if provided
    main_test_results = load_test_results(args.main_results) if args.main_results else {}
    refactor_test_results = load_test_results(args.refactor_results) if args.refactor_results else {}

    # Create comparator
    comparator = OutputComparator(tolerance=args.tolerance)

    # Find all HDF5 files in main directory
    main_h5_files = list(args.main_dir.glob("**/*.h5"))

    if not main_h5_files:
        print(f"‚ùå No HDF5 files found in {args.main_dir}")
        sys.exit(1)

    print(f"Found {len(main_h5_files)} HDF5 files in main branch output")

    # Compare all files
    all_results = []

    for main_file in main_h5_files:
        # Determine relative path
        rel_path = main_file.relative_to(args.main_dir)
        refactor_file = args.refactor_dir / rel_path

        # Extract example name from path
        example_name = str(rel_path.parent) if rel_path.parent != Path('.') else 'root'

        result = comparator.compare_file_pair(main_file, refactor_file, example_name)
        all_results.append(result)

    # Generate report
    report = comparator.generate_report(
        all_results,
        main_test_results,
        refactor_test_results,
        args.report
    )

    # Print summary
    print("\n" + "="*70)
    print("COMPARISON SUMMARY")
    print("="*70)

    identical = sum(1 for r in all_results if r['status'] == 'IDENTICAL')
    numerically_close = sum(1 for r in all_results if r['status'] == 'NUMERICALLY_CLOSE')
    different = sum(1 for r in all_results if r['status'] == 'DIFFERENT')
    errors = sum(1 for r in all_results if 'ERROR' in r['status'] or 'MISSING' in r['status'])

    print(f"‚úÖ Identical: {identical}/{len(all_results)}")
    print(f"‚úÖ Numerically close: {numerically_close}/{len(all_results)}")
    print(f"‚ö†Ô∏è Different: {different}/{len(all_results)}")
    print(f"‚ùå Errors: {errors}/{len(all_results)}")

    if (identical + numerically_close) == len(all_results):
        print("\nüéâ ALL FILES MATCH - SAFE TO MERGE")
        sys.exit(0)
    else:
        print(f"\n‚ö†Ô∏è {different + errors} FILES HAVE ISSUES - REVIEW REQUIRED")
        sys.exit(1)


if __name__ == "__main__":
    main()
